{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6dc366fc",
      "metadata": {
        "id": "6dc366fc"
      },
      "source": [
        "## Semantic Processing Challenge: Word Sense Disambiguation (WSD) with Lesk and WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a13b3c3",
      "metadata": {
        "id": "8a13b3c3"
      },
      "source": [
        "In this notebook, we will build upon our previous work to dive into **Semantic Processing** by exploring **Word Sense Disambiguation (WSD)**, the computational problem of determining which sense (meaning) of a word is activated by its use in a particular context. We will focus on **knowledge-based WSD** using the **WordNet** lexical database and implement the classic **Lesk Algorithm**. Crucially, we'll integrate `spaCy` for automated **Part-of-Speech (POS)** tagging, a vital step that significantly improves the accuracy of WordNet lookups.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ubfvbJ7mmfeV",
      "metadata": {
        "id": "ubfvbJ7mmfeV"
      },
      "source": [
        "#### Note:\n",
        "\n",
        "As you have seen in the previous challenge, we will use our email thread dataset that has 4167 threads and 21684 emails. Like the previous challenges, we will only use a small sample of the dataset and focus on exploring the concepts of semantic processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1fe99c",
      "metadata": {
        "id": "6d1fe99c"
      },
      "source": [
        "#### We will cover the following items as part of this notebook\n",
        "\n",
        "- **Word Sense Disambiguation (WSD)**: Definition and challenge.\n",
        "\n",
        "- **Lesk Algorithm Implementation**: Step-by-step WSD based on context and sense definitions.\n",
        "\n",
        "- **Integration with spaCy**: Using automated POS tags to filter WordNet senses.\n",
        "\n",
        "- **WordNet Similarity Measures**: Applying similarity to compare sense definitions.\n",
        "\n",
        "- **Practical Disambiguation**: Applying the combined method to example sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce663bb",
      "metadata": {
        "id": "3ce663bb"
      },
      "source": [
        "#### What we will be learning from this challenge\n",
        "\n",
        "- **WSD** is essential for true language understanding, distinguishing meanings like \"bank\" (river) vs. \"bank\" (financial).\n",
        "\n",
        "- The **Lesk Algorithm** works by finding the maximum lexical overlap between the context of an ambiguous word and the definitions (glosses) of its potential senses.\n",
        "\n",
        "- **POS Tagging** (e.g., distinguishing noun vs. verb) is crucial for drastically reducing the number of candidate senses.\n",
        "\n",
        "- **WordNet** provides the semantic network (senses, glosses, examples) needed for knowledge-based WSD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655okCMHnN01",
      "metadata": {
        "id": "655okCMHnN01"
      },
      "source": [
        "**Let's get started now**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26dd9423",
      "metadata": {
        "id": "26dd9423"
      },
      "source": [
        "### Setup and Prerequisites\n",
        "\n",
        "We will need `spacy` for POS tagging and `nltk` for WordNet and the Lesk implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787a9133",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "787a9133",
        "outputId": "2068b5f7-9b70-421e-b945-ee3e89a7bb3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Install the main libraries\n",
        "! pip install spacy\n",
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d39811",
      "metadata": {
        "id": "54d39811"
      },
      "source": [
        "We also need to download the necessary NLTK resources and a spaCy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f60906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32f60906",
        "outputId": "0e3c77e0-3c47-4a82-dd8f-90ffae33b191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Download the specific spaCy language model\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f4a536",
      "metadata": {
        "id": "17f4a536"
      },
      "source": [
        "**Library Versions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e4e3756",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic//configs/environment.yml\n",
            "Raw data path: /Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/data/raw/\n",
            "Processed data path: /Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/data/processed/\n",
            "Models path: /Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/models/\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "config_path='/Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/'\n",
        "# Load the environment.yml file\n",
        "print (config_path + \"/configs/environment.yml\")\n",
        "with open(config_path + \"/configs/environment.yml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Choose environment (local or aws)\n",
        "env = \"local\"   # or \"aws\"\n",
        "\n",
        "base_path = config[env][\"base_path\"]\n",
        "raw_data_path = base_path + config[env][\"raw_data\"]\n",
        "processed_data_path = base_path + config[env][\"processed_data\"]\n",
        "models_path = base_path + config[env][\"models\"]\n",
        "\n",
        "print(\"Raw data path:\", raw_data_path)\n",
        "print(\"Processed data path:\",  processed_data_path)\n",
        "print(\"Models path:\",  models_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "599c833b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599c833b",
        "outputId": "c5d9f537-f547-4070-e0ee-b5f851298b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Version: 3.8.7\n",
            "NLTK Version: 3.9.2\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from spacy.lang.en import English\n",
        "import json\n",
        "\n",
        "print(f\"spaCy Version: {spacy.__version__}\")\n",
        "print(f\"NLTK Version: {nltk.__version__}\")\n",
        "\n",
        "# Download NLTK resources required for WSD and POS tagging\n",
        "try:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "    # Note: We rely on spaCy for tagging, but include the NLTK POS tagger resources just in case.\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"NLTK download error: {e}\")\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z6oJDMBAn-kq",
      "metadata": {
        "id": "Z6oJDMBAn-kq"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "We will use the same dataset from the previous exercise. However, we will just use a small sample of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "AZZJnM08oU5l",
      "metadata": {
        "id": "AZZJnM08oU5l"
      },
      "outputs": [],
      "source": [
        "# Loading the JSON data\n",
        "email_data = json.load(open(raw_data_path + \"/email_thread_details.json\"))\n",
        "email_summary = json.load(open(raw_data_path + \"/email_thread_summaries.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "CDR4dnbypTXw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDR4dnbypTXw",
        "outputId": "07ea0a61-2daf-4599-9770-6e2f73e9d575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bert sent multiple emails with attached files containing weekend notes for different dates\n"
          ]
        }
      ],
      "source": [
        "## We will pick a random email summary record as our sample text\n",
        "SAMPLE_TEXT = email_summary[100]['summary'].split(\". \")[0]\n",
        "print(SAMPLE_TEXT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aqxxMu-vq7tP",
      "metadata": {
        "id": "aqxxMu-vq7tP"
      },
      "source": [
        "Let us make a small sample subset to apply the semantic processing methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Y7NEmWGuq1oZ",
      "metadata": {
        "id": "Y7NEmWGuq1oZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "sampled_keys = random.sample(list(range(len(email_summary))), 100)\n",
        "\n",
        "sub_email_dataset = [email_summary[k] for k in sampled_keys]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XQSXPsUksRbz",
      "metadata": {
        "id": "XQSXPsUksRbz"
      },
      "source": [
        "We will use this small subset to apply these different semantic processing methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc500517",
      "metadata": {
        "id": "dc500517"
      },
      "source": [
        "### Word Sense Disambiguation (WSD)\n",
        "\n",
        "**Word Sense Disambiguation (WSD)** is the process of identifying which meaning (or sense) of a word is used in a specific context. For example, the word *pitcher* can refer to a *container for liquids* or a *player in baseball*. WSD aims to resolve this ambiguity based on the surrounding text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jtNVXjrJpxt9",
      "metadata": {
        "id": "jtNVXjrJpxt9"
      },
      "source": [
        "Before proceeding with our dataset, let us take some sample examples to understand this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ffd94a",
      "metadata": {
        "id": "e2ffd94a"
      },
      "source": [
        "Consider the ambiguous word bank in these two sentences:\n",
        "\n",
        "```\n",
        "Sentence\t                        Ambiguous Word\t    Correct Sense\n",
        "\"I withdrew money from the bank.\"\t bank\t             Financial Institution (Noun)\n",
        "\"We walked along the river bank.\"\t bank\t             Edge of a river (Noun)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8429a9",
      "metadata": {
        "id": "ce8429a9"
      },
      "source": [
        "The **Lesk Algorithm** is a knowledge-based WSD technique. It finds the correct sense of an ambiguous word by comparing the **dictionary definition (gloss)** of each candidate sense with the context of the ambiguous word. The sense whose gloss shares the most words (has the **maximum lexical overlap**) with the context is selected as the correct one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a9a64e",
      "metadata": {
        "id": "46a9a64e"
      },
      "source": [
        "We use NLTK's built-in Lesk function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e6a40d9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6a40d9b",
        "outputId": "4852530e-a240-4e50-851d-080202abc8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Disambiguation for: 'I withdrew money from the financial institution near the square.' ---\n",
            "Target Word: bank\n",
            "Best Sense: depository_financial_institution.n.01\n",
            "Definition: a financial institution that accepts deposits and channels the money into lending activities\n"
          ]
        }
      ],
      "source": [
        "target_word = \"bank\"\n",
        "context_1 = \"I withdrew money from the financial institution near the square.\"\n",
        "\n",
        "# Apply Lesk's algorithm\n",
        "best_sense_1 = lesk(context_1.split(), target_word)\n",
        "\n",
        "print(f\"--- Disambiguation for: '{context_1}' ---\")\n",
        "print(f\"Target Word: {target_word}\")\n",
        "print(f\"Best Sense: {best_sense_1.name()}\")\n",
        "print(f\"Definition: {best_sense_1.definition()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OWrfMHyjqFL0",
      "metadata": {
        "id": "OWrfMHyjqFL0"
      },
      "source": [
        "Lesk compared the context words ('money', 'financial', 'institution') with the definitions of all senses of 'bank' and found the best overlap with the 'financial institution' sense."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5i4YVBhqQiC",
      "metadata": {
        "id": "f5i4YVBhqQiC"
      },
      "source": [
        "Now, let us apply this method on our dataset to see if there are any disambiguities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dlrrm0wxqPte",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlrrm0wxqPte",
        "outputId": "4650afa2-79a3-43eb-8b2a-e3c96a871bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Disambiguation for: 'David Minns contacted Shari Stack regarding an ISDA Master Agreement with Westpac for Enron Australia Finance Pty Limited. David asked about changes to the credit sheet and requested a copy of the Enron Corp. guarantee. Margaret Lindeman from Westpac sent David a draft of the ISDA Schedule based on the agreement with Enron North America Corp. Shari Stack expressed surprise at Westpac's email and mentioned that she had sent a mark-up of the schedule to Westpac. Sara Shackleton asked if the master agreement had been executed.' ---\n",
            "Target Word: copy\n",
            "Best Sense: replicate.v.02\n",
            "Definition: reproduce or make an exact copy of\n"
          ]
        }
      ],
      "source": [
        "# Let us pick up a sample target word: 'copy'\n",
        "target_word = \"copy\"\n",
        "\n",
        "for context_summary in sub_email_dataset:\n",
        "  if target_word in context_summary['summary'].split():\n",
        "    best_sense = lesk(context_summary['summary'].split(), target_word)\n",
        "\n",
        "    print(f\"--- Disambiguation for: '{context_summary['summary']}' ---\")\n",
        "    print(f\"Target Word: {target_word}\")\n",
        "    print(f\"Best Sense: {best_sense.name()}\")\n",
        "    print(f\"Definition: {best_sense.definition()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb731e5d",
      "metadata": {
        "id": "fb731e5d"
      },
      "source": [
        "#### Integration with spaCy for Automated POS Tagging\n",
        "\n",
        "A word's part-of-speech (e.g., noun vs. verb) significantly limits its potential senses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b45e45",
      "metadata": {
        "id": "89b45e45"
      },
      "source": [
        "```\n",
        "Word\tPOS\t    Senses (WordNet)\n",
        "train\tNoun\tLocomotive, Sequence (of events)\n",
        "train\tVerb\tTo teach, To aim (a weapon)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af71ba30",
      "metadata": {
        "id": "af71ba30"
      },
      "source": [
        "WordNet requires a specific POS format (`wn.NOUN`, `wn.VERB`, etc.). We map spaCy's UPOS tags to the WordNet format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "03d1270b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03d1270b",
        "outputId": "c65b5521-4b73-432d-81a7-b12dc5d26bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified WordNet POS: v\n",
            "\n",
            "--- Disambiguation with spaCy POS Filter ---\n",
            "Target Word: train\n",
            "Best Sense: train.v.08\n",
            "Definition: exercise in order to prepare for an event or competition\n"
          ]
        }
      ],
      "source": [
        "# Function to convert spaCy POS tags to WordNet POS tags\n",
        "def spacy_to_wordnet(spacy_tag):\n",
        "    if spacy_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif spacy_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif spacy_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif spacy_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    return None\n",
        "\n",
        "# Combined Lesk function using spaCy for POS\n",
        "def lesk_with_pos(sentence, ambiguous_word):\n",
        "    # 1. Use spaCy to process the sentence and get POS tags\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # 2. Extract context words (all words except the target word)\n",
        "    context = [token.text for token in doc if token.text.lower() != ambiguous_word.lower()]\n",
        "\n",
        "    # 3. Get the POS tag for the ambiguous word\n",
        "    target_token = [token for token in doc if token.text.lower() == ambiguous_word.lower()][0]\n",
        "    wordnet_pos = spacy_to_wordnet(target_token.pos_)\n",
        "\n",
        "    print(f\"Identified WordNet POS: {wordnet_pos}\")\n",
        "\n",
        "    # 4. Apply NLTK's Lesk algorithm, filtering senses by the POS tag\n",
        "    best_sense = lesk(context, ambiguous_word, pos=wordnet_pos)\n",
        "    return best_sense\n",
        "\n",
        "# Example Sentence\n",
        "sentence_2 = \"The athlete needs to train hard for the Olympics.\"\n",
        "ambiguous_word_2 = \"train\"\n",
        "\n",
        "best_sense_2 = lesk_with_pos(sentence_2, ambiguous_word_2)\n",
        "\n",
        "print(\"\\n--- Disambiguation with spaCy POS Filter ---\")\n",
        "print(f\"Target Word: {ambiguous_word_2}\")\n",
        "print(f\"Best Sense: {best_sense_2.name()}\")\n",
        "print(f\"Definition: {best_sense_2.definition()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HRyofH-QvD3T",
      "metadata": {
        "id": "HRyofH-QvD3T"
      },
      "source": [
        "spaCy correctly identifies 'train' as a VERB. Lesk then only considers the verb senses of 'train', significantly improving the chance of selecting the correct sense ('to teach or discipline')."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ij0YVzuPvcFT",
      "metadata": {
        "id": "Ij0YVzuPvcFT"
      },
      "source": [
        "Next, let us try this on our sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Yq1yXZ5tvgcg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq1yXZ5tvgcg",
        "outputId": "17a938a3-58a8-4a92-84e3-97e3b5e22dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified WordNet POS: n\n",
            "--- Disambiguation with spaCy POS Filter: 'David Minns contacted Shari Stack regarding an ISDA Master Agreement with Westpac for Enron Australia Finance Pty Limited. David asked about changes to the credit sheet and requested a copy of the Enron Corp. guarantee. Margaret Lindeman from Westpac sent David a draft of the ISDA Schedule based on the agreement with Enron North America Corp. Shari Stack expressed surprise at Westpac's email and mentioned that she had sent a mark-up of the schedule to Westpac. Sara Shackleton asked if the master agreement had been executed.' ---\n",
            "Target Word: copy\n",
            "Best Sense: transcript.n.02\n",
            "Definition: a reproduction of a written record (e.g. of a legal or school record)\n"
          ]
        }
      ],
      "source": [
        "# Let us pick up a sample target word: 'copy'\n",
        "target_word = \"copy\"\n",
        "\n",
        "for context_summary in sub_email_dataset:\n",
        "  if target_word in context_summary['summary'].split():\n",
        "    best_sense = lesk_with_pos(context_summary['summary'], target_word)\n",
        "\n",
        "    print(f\"--- Disambiguation with spaCy POS Filter: '{context_summary['summary']}' ---\")\n",
        "    print(f\"Target Word: {target_word}\")\n",
        "    print(f\"Best Sense: {best_sense.name()}\")\n",
        "    print(f\"Definition: {best_sense.definition()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea0c767",
      "metadata": {
        "id": "4ea0c767"
      },
      "source": [
        "#### WordNet Similarity Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e089ca0",
      "metadata": {
        "id": "0e089ca0"
      },
      "source": [
        "WordNet is a semantic network, connecting senses through relations like hypernymy (is-a relation) and meronymy (part-of relation). We can use these relations to calculate similarity between senses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32f9b27",
      "metadata": {
        "id": "f32f9b27"
      },
      "source": [
        "We'll compare the selected sense of \"train\" (verb) with a different verb sense, like train.v.03 (\"to aim a weapon\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "71916ce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71916ce8",
        "outputId": "ad3c630d-02d4-4ec7-f8cb-16bbb6e807df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- WordNet Similarity Comparison ---\n",
            "Sense A (Lesk): train.v.08 | Def: exercise in order to prepare for an event or competition\n",
            "Sense B (Alternative V): discipline.v.01 | Def: develop (children's) behavior by instruction and practice; especially to teach self-control\n",
            "Sense C (Incorrect N): train.n.01 | Def: public transport provided by a line of railway cars coupled together and drawn by a locomotive\n",
            "\n",
            "Path Similarity (A vs B - V vs V): 0.17\n"
          ]
        }
      ],
      "source": [
        "# Get the sense chosen by Lesk\n",
        "sense_A = best_sense_2\n",
        "\n",
        "# Define an alternative sense (e.g., 'aim a weapon')\n",
        "sense_B = wn.synset('train.v.03') # to aim or direct (something, as a gun or camera)\n",
        "\n",
        "# Get the third, incorrect sense (e.g., 'locomotive')\n",
        "sense_C = wn.synset('train.n.01')\n",
        "\n",
        "print(\"--- WordNet Similarity Comparison ---\")\n",
        "print(f\"Sense A (Lesk): {sense_A.name()} | Def: {sense_A.definition()}\")\n",
        "print(f\"Sense B (Alternative V): {sense_B.name()} | Def: {sense_B.definition()}\")\n",
        "print(f\"Sense C (Incorrect N): {sense_C.name()} | Def: {sense_C.definition()}\")\n",
        "\n",
        "# Calculate the path similarity (based on the shortest path in the hierarchy)\n",
        "# Note: Similarity is only calculated between senses of the same POS\n",
        "similarity_AB = sense_A.path_similarity(sense_B)\n",
        "# similarity_AC will fail or yield low/zero value because they are different POS/hierarchies\n",
        "\n",
        "print(f\"\\nPath Similarity (A vs B - V vs V): {similarity_AB:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RAQmr7YwWLk",
      "metadata": {
        "id": "6RAQmr7YwWLk"
      },
      "source": [
        "Path similarity measures semantic closeness. A higher score (closer to 1.0) indicates that the senses are functionally or conceptually closer within the WordNet hierarchy. This is often used to verify the output of Lesk or to refine WSD when multiple senses yield similar overlap scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f762384",
      "metadata": {
        "id": "9f762384"
      },
      "source": [
        "#### Practical Disambiguation\n",
        "\n",
        "Let's apply the combined `lesk_with_pos` function to distinguish between the two meanings of **copy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fa43ba18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa43ba18",
        "outputId": "c00aa3a0-39ae-40e9-c2d1-4b306baaabfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Practical Disambiguation of 'copy' ---\n",
            "Identified WordNet POS: n\n",
            "\n",
            "Sentence: 'Sara requested for a copy.'\n",
            "Selected Sense: copy.n.04\n",
            "Definition: material suitable for a journalistic account\n",
            "--------------------\n",
            "Identified WordNet POS: v\n",
            "\n",
            "Sentence: 'Enron employees transferring to UBS Warburg Energy are required to copy all documents themselves.'\n",
            "Selected Sense: copy.v.01\n",
            "Definition: copy down as is\n"
          ]
        }
      ],
      "source": [
        "target_word = \"copy\"\n",
        "\n",
        "# Sentence 3: Document or book context (Noun)\n",
        "sentence_3 = \"Sara requested for a copy.\"\n",
        "\n",
        "# Sentence 4:  Transfering documents context (Verb)\n",
        "sentence_4 = \"Enron employees transferring to UBS Warburg Energy are required to copy all documents themselves.\"\n",
        "\n",
        "print(\"\\n--- Practical Disambiguation of 'copy' ---\")\n",
        "\n",
        "# Disambiguation 1: Noun Context\n",
        "best_sense_3 = lesk_with_pos(sentence_3, target_word)\n",
        "print(f\"\\nSentence: '{sentence_3}'\")\n",
        "print(f\"Selected Sense: {best_sense_3.name()}\")\n",
        "print(f\"Definition: {best_sense_3.definition()}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Disambiguation 2: Verb\n",
        "best_sense_4 = lesk_with_pos(sentence_4, target_word)\n",
        "print(f\"\\nSentence: '{sentence_4}'\")\n",
        "print(f\"Selected Sense: {best_sense_4.name()}\")\n",
        "print(f\"Definition: {best_sense_4.definition()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b455585d",
      "metadata": {
        "id": "b455585d"
      },
      "source": [
        "#### Conclusion\n",
        "\n",
        "In this challenge, we successfully implemented **Word Sense Disambiguation** using the **Lesk Algorithm**, significantly enhanced by `spaCy's` automated POS tagging. By using the context of the sentence (the surrounding words) and comparing it to the definitions provided by WordNet, we were able to computationally distinguish between multiple meanings of ambiguous words like \"copy\" and \"train.\"\n",
        "\n",
        "This notebook demonstrates a foundational technique in **Semantic Processing**, highlighting how knowledge-based resources (WordNet) combined with robust NLP tools (spaCy, NLTK) allow machines to move beyond mere syntax and begin to grasp the meaning of text. WSD is a critical component for tasks like Machine Translation, Question Answering, and Information Retrieval, where understanding the precise intended meaning is paramount."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caef14bf",
      "metadata": {
        "id": "caef14bf"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
