{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "422917ea",
      "metadata": {
        "id": "422917ea"
      },
      "source": [
        "## Distributional Semantics: Co-Occurrence Matrices and PMI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6741c3a",
      "metadata": {
        "id": "b6741c3a"
      },
      "source": [
        "This exercise explores **Distributional Semantics**, the principle that \"a word is characterised by the company it keeps\". We will construct a numerical representation of this idea by creating a **Co-occurrence Matrix** from a sample corpus. To extract meaningful relationships beyond simple frequency, we will then compute **Pointwise Mutual Information (PMI) scores**. This method is foundational to understanding modern word embeddings (like **Word2Vec**) as it quantifies the strength of association between words based on their observed co-occurrences. Like the previous exercises, we will use the same email summary dataset to learn the concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e251f41c",
      "metadata": {
        "id": "e251f41c"
      },
      "source": [
        "#### We will cover the following topics as part of this exercise\n",
        "\n",
        "- **Distributional Hypothesis**: Definition and core concept.\n",
        "\n",
        "- **Co-occurrence Matrix Construction**: Generating counts using a context window.\n",
        "\n",
        "- **Pointwise Mutual Information (PMI)**: Definition and calculation.\n",
        "\n",
        "- **Comparative Analysis**: Contrasting raw counts with PMI scores.\n",
        "\n",
        "- **Practical Application**: Illustrating word connections via the final matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "460ff440",
      "metadata": {
        "id": "460ff440"
      },
      "source": [
        "#### What we will learn from this exercise:\n",
        "\n",
        "- The **Distributional Hypothesis** states that words with similar meanings appear in similar contexts.\n",
        "\n",
        "- A **Co-occurrence Matrix** is the tabular representation of how often words appear together within a specified context window.\n",
        "\n",
        "- **PMI** is a statistical measure used to quantify the strength of association between two words, prioritising interesting co-occurrences over merely frequent ones.\n",
        "\n",
        "- High **PMI values reveal meaningful semantic relationships between words**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-vkbIjeJooB",
      "metadata": {
        "id": "V-vkbIjeJooB"
      },
      "source": [
        "#### Note:\n",
        "\n",
        "Like the previous exercises, we will use our email thread dataset that has 4167 threads and 21684 emails. We will only use a small sample of the dataset and focus on exploring the concepts of distributional semantics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Ijm2RKMJzba",
      "metadata": {
        "id": "3Ijm2RKMJzba"
      },
      "source": [
        "**Let's get started now**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287e7104",
      "metadata": {
        "id": "287e7104"
      },
      "source": [
        "#### Setup and Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db81872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0db81872",
        "outputId": "bda753b9-d721-46a9-835a-0130714c315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2915a4a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic//configs/environment.yml\n",
            "Raw data path: /Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/data/raw/\n",
            "Processed data path: /Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/data/processed/\n",
            "Models path: /Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/models/\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "config_path='/Users/aditikulkarni/Documents/Masters/AI-Projects/05-DL-NLP/nlp-semantic/'\n",
        "# Load the environment.yml file\n",
        "print (config_path + \"/configs/environment.yml\")\n",
        "with open(config_path + \"/configs/environment.yml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Choose environment (local or aws)\n",
        "env = \"local\"   # or \"aws\"\n",
        "\n",
        "base_path = config[env][\"base_path\"]\n",
        "raw_data_path = base_path + config[env][\"raw_data\"]\n",
        "processed_data_path = base_path + config[env][\"processed_data\"]\n",
        "models_path = base_path + config[env][\"models\"]\n",
        "\n",
        "print(\"Raw data path:\", raw_data_path)\n",
        "print(\"Processed data path:\",  processed_data_path)\n",
        "print(\"Models path:\",  models_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "120abd44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "120abd44",
        "outputId": "0105491b-5c64-4cd7-8663-4e2c7a204ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy Version: 2.3.1\n",
            "Pandas Version: 2.3.1\n",
            "NLTK Version: 3.9.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from math import log\n",
        "import json\n",
        "\n",
        "print(f\"NumPy Version: {np.__version__}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n",
        "print(f\"NLTK Version: {nltk.__version__}\")\n",
        "\n",
        "# Download NLTK resources required for tokenization\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except Exception as e:\n",
        "    print(f\"NLTK download error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc36ba3",
      "metadata": {
        "id": "dcc36ba3"
      },
      "source": [
        "**Sample Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20919ece",
      "metadata": {
        "id": "20919ece",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "We create a small corpus from our email dataset to clearly illustrate the counting process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "52a2i_93J8aJ",
      "metadata": {
        "id": "52a2i_93J8aJ"
      },
      "outputs": [],
      "source": [
        "# Loading the JSON data\n",
        "email_data = json.load(open(raw_data_path + \"email_thread_details.json\"))\n",
        "email_summary = json.load(open(raw_data_path + \"email_thread_summaries.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0-ypWFE3KTua",
      "metadata": {
        "id": "0-ypWFE3KTua"
      },
      "outputs": [],
      "source": [
        "# Let's select some more samples to create our small corpus\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(123) # Set a random seed for reproducibility\n",
        "sampled_keys = random.sample(list(range(len(email_summary))), 2)\n",
        "\n",
        "sub_email_dataset = [email_summary[k]['summary'] for k in sampled_keys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ptCAHk5IKcvj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptCAHk5IKcvj",
        "outputId": "a54bc4c6-c98e-4952-e933-ae9df3042419"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Clayton asks Chris for names and telephone numbers of individuals at various pipelines who can provide him with FT and IT charges, both peak and off-peak, going back 4 years. He mentions that these individuals should be cool with Enron and/or Chris's name. Chris agrees to help and asks Clayton to email him the specific information he needs. Clayton expresses his gratitude and offers to treat Chris to a nice lunch as a favor.\",\n",
              " 'The email thread covers various topics, including trading, relationships, hiring a call girl, hurricanes, and making plans for a Saturday night. The first email asks about someone\\'s trading activities. The second email expresses a desire for a girlfriend after marriage. The third email discusses hiring a call girl for specific fantasies and asks about the cost. The fourth email refers to someone as a home wrecker. The fifth email hopes the recipient avoids a tropical storm in the Gulf and mentions the movie \"The Perfect Storm.\" The sixth email agrees to meet up after the recipient returns and hopes they avoid hurricanes. The final email suggests doing a mud pie activity and confirms plans for Saturday night.']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_email_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0OgoO0kKLpcy",
      "metadata": {
        "id": "0OgoO0kKLpcy"
      },
      "source": [
        "Since, the summaries are very long and consist of multiple sentences, let us take only one summary and break that into a small corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "pssvyMi0LorJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pssvyMi0LorJ",
        "outputId": "45aef537-170c-4cfb-8a5b-46c354e9702e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The email thread covers various topics, including trading, relationships, hiring a call girl, hurricanes, and making plans for a Saturday night',\n",
              " \"The first email asks about someone's trading activities\",\n",
              " 'The second email expresses a desire for a girlfriend after marriage',\n",
              " 'The third email discusses hiring a call girl for specific fantasies and asks about the cost',\n",
              " 'The fourth email refers to someone as a home wrecker',\n",
              " 'The fifth email hopes the recipient avoids a tropical storm in the Gulf and mentions the movie \"The Perfect Storm.\" The sixth email agrees to meet up after the recipient returns and hopes they avoid hurricanes',\n",
              " 'The final email suggests doing a mud pie activity and confirms plans for Saturday night.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CORPUS = sub_email_dataset[1].split(\". \")\n",
        "CORPUS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jg26lYbmNb2S",
      "metadata": {
        "id": "jg26lYbmNb2S"
      },
      "source": [
        "Let us build the vocabulary, which is the list of unique words present in out corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "FHR_-sLHMvq1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHR_-sLHMvq1",
        "outputId": "aef3ff2b-d13b-4083-a53d-0f416ff7ca02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['girl,',\n",
              " 'activities',\n",
              " 'various',\n",
              " 'home',\n",
              " 'agrees',\n",
              " 'and',\n",
              " 'trading,',\n",
              " 'up',\n",
              " 'suggests',\n",
              " 'activity',\n",
              " 'fantasies',\n",
              " 'asks',\n",
              " 'fourth',\n",
              " 'to',\n",
              " 'avoids',\n",
              " 'someone',\n",
              " 'returns',\n",
              " 'they',\n",
              " 'Gulf',\n",
              " 'including',\n",
              " 'for',\n",
              " 'storm',\n",
              " 'hurricanes,',\n",
              " 'about',\n",
              " 'discusses',\n",
              " 'sixth',\n",
              " 'call',\n",
              " 'Perfect',\n",
              " 'expresses',\n",
              " 'cost',\n",
              " '\"The',\n",
              " 'plans',\n",
              " 'final',\n",
              " 'fifth',\n",
              " 'as',\n",
              " 'trading',\n",
              " 'topics,',\n",
              " 'specific',\n",
              " 'The',\n",
              " 'second',\n",
              " 'Saturday',\n",
              " 'the',\n",
              " 'recipient',\n",
              " 'girl',\n",
              " 'third',\n",
              " 'avoid',\n",
              " 'tropical',\n",
              " 'refers',\n",
              " 'mentions',\n",
              " 'Storm.\"',\n",
              " 'email',\n",
              " 'night',\n",
              " 'a',\n",
              " 'making',\n",
              " 'movie',\n",
              " 'covers',\n",
              " 'first',\n",
              " 'girlfriend',\n",
              " 'hurricanes',\n",
              " 'doing',\n",
              " 'in',\n",
              " 'pie',\n",
              " 'confirms',\n",
              " \"someone's\",\n",
              " 'night.',\n",
              " 'hiring',\n",
              " 'desire',\n",
              " 'hopes',\n",
              " 'mud',\n",
              " 'after',\n",
              " 'marriage',\n",
              " 'meet',\n",
              " 'wrecker',\n",
              " 'thread',\n",
              " 'relationships,']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB = set()\n",
        "for sentence in CORPUS:\n",
        "  for word in sentence.split():\n",
        "    VOCAB.add(word)\n",
        "\n",
        "VOCAB = list(VOCAB)\n",
        "VOCAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6578da02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6578da02",
        "outputId": "1ecd6823-84ad-42c4-cf9e-37bb8c20f095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Corpus and Parameters ---\n",
            "Total Sentences: 7\n",
            "Total Vocabulary Size: 75\n",
            "Context Window Size: 2 (meaning 2 words to the left and 2 to the right)\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "CONTEXT_WINDOW = 2\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "\n",
        "print(\"\\n--- Corpus and Parameters ---\")\n",
        "print(f\"Total Sentences: {len(CORPUS)}\")\n",
        "print(f\"Total Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(f\"Context Window Size: {CONTEXT_WINDOW} (meaning 2 words to the left and 2 to the right)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f040ea8c",
      "metadata": {
        "id": "f040ea8c"
      },
      "source": [
        "#### Distributional Hypothesis\n",
        "\n",
        "The **Distributional Hypothesis** is the central idea behind distributional semantics: **words that occur in similar contexts tend to have similar meanings**. For instance, the words \"car\" and \"truck\" might both frequently co-occur with \"drive,\" \"engine,\" and \"road,\" suggesting they are semantically related (vehicles)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c82b38",
      "metadata": {
        "id": "69c82b38"
      },
      "source": [
        "**Preprocessing and Vocabulary**:\n",
        "We have seen a simple way to build the vocabulary. But, if you remember from our lexical processing exercise, we need to perform additional pre-processing steps like case conversion, tokenisation, extracting only words and etc. to build a noise free vocabulary.\n",
        "\n",
        "So, next, we will tokenise the corpus and build a unique vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2c32f0f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c32f0f9",
        "outputId": "15611ab6-f77c-4819-baad-517d8f8dc52a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vocabulary Size: 67\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'activities',\n",
              " 'activity',\n",
              " 'after',\n",
              " 'agrees',\n",
              " 'and',\n",
              " 'as',\n",
              " 'asks',\n",
              " 'avoid',\n",
              " 'avoids',\n",
              " 'call',\n",
              " 'confirms',\n",
              " 'cost',\n",
              " 'covers',\n",
              " 'desire',\n",
              " 'discusses',\n",
              " 'doing',\n",
              " 'email',\n",
              " 'expresses',\n",
              " 'fantasies',\n",
              " 'fifth',\n",
              " 'final',\n",
              " 'first',\n",
              " 'for',\n",
              " 'fourth',\n",
              " 'girl',\n",
              " 'girlfriend',\n",
              " 'gulf',\n",
              " 'hiring',\n",
              " 'home',\n",
              " 'hopes',\n",
              " 'hurricanes',\n",
              " 'in',\n",
              " 'including',\n",
              " 'making',\n",
              " 'marriage',\n",
              " 'meet',\n",
              " 'mentions',\n",
              " 'movie',\n",
              " 'mud',\n",
              " 'night',\n",
              " 'perfect',\n",
              " 'pie',\n",
              " 'plans',\n",
              " 'recipient',\n",
              " 'refers',\n",
              " 'relationships',\n",
              " 'returns',\n",
              " 'saturday',\n",
              " 'second',\n",
              " 'sixth',\n",
              " 'someone',\n",
              " 'specific',\n",
              " 'storm',\n",
              " 'suggests',\n",
              " 'the',\n",
              " 'they',\n",
              " 'third',\n",
              " 'thread',\n",
              " 'to',\n",
              " 'topics',\n",
              " 'trading',\n",
              " 'tropical',\n",
              " 'up',\n",
              " 'various',\n",
              " 'wrecker']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenise and flatten the corpus\n",
        "tokenized_corpus = [word_tokenize(sent.lower()) for sent in CORPUS]\n",
        "all_words = [word for sent in tokenized_corpus for word in sent if word.isalnum()]\n",
        "\n",
        "# Create unique vocabulary and word-to-index mapping\n",
        "vocabulary = sorted(list(set(all_words)))\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "index_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
        "VOCAB_SIZE = len(vocabulary)\n",
        "\n",
        "print(f\"\\nVocabulary Size: {VOCAB_SIZE}\")\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee3856d",
      "metadata": {
        "id": "0ee3856d"
      },
      "source": [
        "#### Co-occurrence Matrix Construction\n",
        "\n",
        "A Co-occurrence Matrix M is a square matrix where rows and columns represent words in the vocabulary. The entry $M_{ij}$ counts how many times word $w_i$ and word $w_j$ appear together within the defined context window across the entire corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0e575d",
      "metadata": {
        "id": "4f0e575d"
      },
      "source": [
        "**Implementation**: We iterate through the corpus and increment counts for co-occurring pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3b259c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b259c0c",
        "outputId": "eb74de10-6979-4466-a40a-52dce1ed6b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Raw Co-occurrence Matrix (partial display)---\n",
            "            a  about  activities  activity  after\n",
            "a           0      0           0         0      1\n",
            "about       0      0           0         0      0\n",
            "activities  0      0           0         0      0\n",
            "activity    0      0           0         0      0\n",
            "after       1      0           0         0      0\n"
          ]
        }
      ],
      "source": [
        "# Initialise the co-occurrence matrix with zeros\n",
        "co_occurrence_matrix = np.zeros((VOCAB_SIZE, VOCAB_SIZE), dtype=np.int32)\n",
        "\n",
        "# Build the matrix\n",
        "for sentence in tokenized_corpus:\n",
        "    # Remove punctuation/non-alpha words for simplicity\n",
        "    clean_sentence = [word for word in sentence if word in word_to_index]\n",
        "\n",
        "    for i, target_word in enumerate(clean_sentence):\n",
        "        target_index = word_to_index[target_word]\n",
        "\n",
        "        # Define the context boundary (excluding the target word itself)\n",
        "        start = max(0, i - CONTEXT_WINDOW)\n",
        "        end = min(len(clean_sentence), i + CONTEXT_WINDOW + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if i != j: # Ensure we don't count self-co-occurrences\n",
        "                context_word = clean_sentence[j]\n",
        "                context_index = word_to_index[context_word]\n",
        "\n",
        "                # Increment the count for the pair (target_word, context_word)\n",
        "                co_occurrence_matrix[target_index, context_index] += 1\n",
        "\n",
        "# Convert to a DataFrame for clear visualization\n",
        "co_occurrence_df = pd.DataFrame(\n",
        "    co_occurrence_matrix,\n",
        "    index=vocabulary,\n",
        "    columns=vocabulary\n",
        ")\n",
        "\n",
        "print(\"\\n--- Raw Co-occurrence Matrix (partial display)---\")\n",
        "print(co_occurrence_df.head(5).iloc[:, 0:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8VhikE0zOsn6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VhikE0zOsn6",
        "outputId": "a5dedfd2-8cf5-4ee0-8f42-6b7262b1631c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Raw Count for ('email', 'thread'): 1\n"
          ]
        }
      ],
      "source": [
        "# Inspect a frequent pair:\n",
        "pair = ('email', 'thread')\n",
        "q_idx = word_to_index[pair[0]]\n",
        "f_idx = word_to_index[pair[1]]\n",
        "print(f\"\\nRaw Count for ('{pair[0]}', '{pair[1]}'): {co_occurrence_matrix[q_idx, f_idx]}\")\n",
        "\n",
        "# Normalisation (optional, but standard for some models)\n",
        "# L2 Normalisation (vector length):\n",
        "# co_occurrence_normalized = co_occurrence_matrix / np.linalg.norm(co_occurrence_matrix, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c22effdd",
      "metadata": {
        "id": "c22effdd"
      },
      "source": [
        "#### Pointwise Mutual Information (PMI)\n",
        "\n",
        "**Pointwise Mutual Information (PMI)** measures the dependency between two random variables (in this case, two words). It quantifies how much more often two words $w_1$​ and $w_2$ co-occur than would be expected by chance, assuming independence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "571fd08c",
      "metadata": {
        "id": "571fd08c"
      },
      "source": [
        "$$\\text{PMI}(w_1, w_2) = \\log_2 \\left( \\frac{P(w_1, w_2)}{P(w_1) P(w_2)} \\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38d9fb68",
      "metadata": {
        "id": "38d9fb68"
      },
      "source": [
        "Where:\n",
        "\n",
        "- $P(w_1 , w_2)$ is the **joint probability** of word $w_1$ and word $w_2$ co-occurring.\n",
        "- $P(w_1)$ and $P(w_2)$ are the **individual probabilities** of $w_1$ and $w_2$ occurring in the context, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379bbfdb",
      "metadata": {
        "id": "379bbfdb"
      },
      "source": [
        "**PMI Calculation**: We first calculate the necessary probabilities from the raw counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "438a746a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "438a746a",
        "outputId": "0695b9a5-9e6b-44a7-c2a7-baf6bee0d5a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Positive PMI (PPMI) Matrix (Sample) ---\n",
            "               a  about  activities  activity  after\n",
            "a           0.00    0.0         0.0       0.0   0.93\n",
            "about       0.00    0.0         0.0       0.0   0.00\n",
            "activities  0.00    0.0         0.0       0.0   0.00\n",
            "activity    0.00    0.0         0.0       0.0   0.00\n",
            "after       0.93    0.0         0.0       0.0   0.00\n"
          ]
        }
      ],
      "source": [
        "# Total number of co-occurrences in the matrix (sum of all entries)\n",
        "total_co_occurrences = np.sum(co_occurrence_matrix)\n",
        "\n",
        "# Word frequency (sum of counts across rows/columns) - approximates P(w)\n",
        "word_frequencies = np.sum(co_occurrence_matrix, axis=1)\n",
        "\n",
        "# Initialise the PMI matrix\n",
        "pmi_matrix = np.zeros((VOCAB_SIZE, VOCAB_SIZE))\n",
        "\n",
        "for i in range(VOCAB_SIZE):\n",
        "    for j in range(VOCAB_SIZE):\n",
        "        raw_count = co_occurrence_matrix[i, j]\n",
        "\n",
        "        # Only proceed if there is at least one co-occurrence (to avoid log(0))\n",
        "        if raw_count > 0:\n",
        "            # 1. P(w_i, w_j): Joint Probability\n",
        "            p_ij = raw_count / total_co_occurrences\n",
        "\n",
        "            # 2. P(w_i) and P(w_j): Individual Probabilities (approximation)\n",
        "            p_i = word_frequencies[i] / total_co_occurrences\n",
        "            p_j = word_frequencies[j] / total_co_occurrences\n",
        "\n",
        "            # 3. Calculate PMI\n",
        "            pmi = log(p_ij / (p_i * p_j), 2)\n",
        "            pmi_matrix[i, j] = pmi\n",
        "        else:\n",
        "            # Common technique: use Positive PMI (PPMI), setting negative values to zero.\n",
        "            pmi_matrix[i, j] = 0\n",
        "\n",
        "# Convert to a DataFrame for visualisation\n",
        "pmi_df = pd.DataFrame(\n",
        "    pmi_matrix,\n",
        "    index=vocabulary,\n",
        "    columns=vocabulary\n",
        ")\n",
        "\n",
        "print(\"\\n--- Positive PMI (PPMI) Matrix (Sample) ---\")\n",
        "print(pmi_df.head(5).iloc[:, 0:5].round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e05d6d",
      "metadata": {
        "id": "07e05d6d"
      },
      "source": [
        "#### Comparative Analysis: Raw Co-occurrence vs. PMI\n",
        "\n",
        "Raw co-occurrence counts favor frequent words (e.g., \"the,\" \"a\"). PMI, however, highlights unexpected and semantically meaningful co-occurrences.\n",
        "\n",
        "**Inspecting Relationships** : Let's compare the count and PMI for common (less informative) and specific (more informative) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fad85a51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fad85a51",
        "outputId": "61b7738c-ff7d-4bb1-c337-9f6ffc215b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Comparison: Raw Counts vs. PMI ---\n",
            "Pair            | Raw Count  | PMI Score \n",
            "----------------------------------------\n",
            "(the, email):15 |          9 |       1.59\n",
            "(fourth, movie):15 |          0 |       0.00\n",
            "(making, plans):15 |          1 |       3.73\n"
          ]
        }
      ],
      "source": [
        "pairs_to_compare = [\n",
        "    ('the', 'email'),   # Common function word + specific word\n",
        "    ('fourth', 'movie'), # Descriptive word + specific word\n",
        "    ('making', 'plans')  # Strong verb-noun relationship\n",
        "]\n",
        "\n",
        "print(\"\\n--- Comparison: Raw Counts vs. PMI ---\")\n",
        "print(f\"{'Pair':15} | {'Raw Count':10} | {'PMI Score':10}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for w1, w2 in pairs_to_compare:\n",
        "    idx1 = word_to_index[w1]\n",
        "    idx2 = word_to_index[w2]\n",
        "\n",
        "    raw = co_occurrence_matrix[idx1, idx2]\n",
        "    pmi = pmi_matrix[idx1, idx2]\n",
        "\n",
        "    print(f\"({w1}, {w2}):15 | {raw:10} | {pmi:10.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o2YbKOteQhmU",
      "metadata": {
        "id": "o2YbKOteQhmU"
      },
      "source": [
        "1. **('the', 'email')**: High **Raw Count (9)** due to the frequent word *'the'*, but only **moderate PMI (1.59)** since *'the'* co-occurs widely and isn’t strongly linked to *'email'*.\n",
        "\n",
        "2. **('fourth', 'movie')**: **Raw Count (0)** and **PMI (0.00)** indicate no meaningful co-occurrence.\n",
        "\n",
        "3. **('making', 'plans')**: **Low Raw Count (1)** but **high PMI (3.73)**, showing a strong, meaningful association between the two words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c598c5",
      "metadata": {
        "id": "88c598c5"
      },
      "source": [
        "#### Practical Application: Revealing Word Connections\n",
        "\n",
        "The final PMI matrix serves as a vector-based representation of word meaning. By looking at the column (or row) vectors, we can see the words that are most strongly associated with a target word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "10bf2880",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10bf2880",
        "outputId": "0413e6c9-0c6c-4829-ca95-3a6947d91505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Words Most Associated with 'email' (by PMI) ---\n",
            "second    2.2\n",
            "third     2.2\n",
            "fourth    2.2\n",
            "first     2.2\n",
            "final     2.2\n",
            "Name: email, dtype: float64\n",
            "\n",
            "--- Words Most Associated with 'confirms' (by PMI) ---\n",
            "activity    4.73\n",
            "plans       3.73\n",
            "for         2.73\n",
            "and         2.41\n",
            "a           0.00\n",
            "Name: confirms, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "target_word = 'email'\n",
        "\n",
        "# Get the PMI vector for the target word\n",
        "pmi_vector = pmi_df.loc[target_word]\n",
        "\n",
        "# Find the words with the highest PMI scores with the target\n",
        "most_associated_words = pmi_vector.sort_values(ascending=False).drop(target_word).head(5)\n",
        "\n",
        "print(f\"\\n--- Words Most Associated with '{target_word}' (by PMI) ---\")\n",
        "print(most_associated_words.round(2))\n",
        "\n",
        "target_word_2 = 'confirms'\n",
        "pmi_vector_2 = pmi_df.loc[target_word_2]\n",
        "most_associated_words_2 = pmi_vector_2.sort_values(ascending=False).drop(target_word_2).head(5)\n",
        "\n",
        "print(f\"\\n--- Words Most Associated with '{target_word_2}' (by PMI) ---\")\n",
        "print(most_associated_words_2.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c5a09c",
      "metadata": {
        "id": "f3c5a09c"
      },
      "source": [
        "#### Conclusion\n",
        "\n",
        "This exercise successfully demonstrated the foundational steps of Distributional Semantics. We moved from a raw text corpus to a numerical representation by:\n",
        "\n",
        "1. Constructing a Co-occurrence Matrix using a defined context window.\n",
        "\n",
        "2. Computing the Pointwise Mutual Information (PMI) score.\n",
        "\n",
        "By comparing raw counts and PMI, we saw that raw counts are skewed by high-frequency words, while PMI effectively filters for the most meaningful semantic associations. This technique, often refined using dimensionality reduction, forms the basis of classic word embedding models, proving that word meaning can be effectively encoded by observing the patterns of their neighbours."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9361496",
      "metadata": {
        "id": "d9361496"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
