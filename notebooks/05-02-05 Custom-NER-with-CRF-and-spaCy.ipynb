{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf95321d",
      "metadata": {
        "id": "cf95321d"
      },
      "source": [
        "### Custom Named Entity Recognition (NER) with CRF and spaCy\n",
        "\n",
        "Continuing from our previous exercise, in this exercise, we will focus on extending **Named Entity Recognition (NER)** capabilities beyond pre-trained models. We will explore two primary methods for Custom NER:\n",
        "\n",
        "- **Conditional Random Fields (CRF)**: A traditional, powerful sequence tagging model requiring custom feature engineering. We will cover the data preparation and feature extraction process critical for CRF training.\n",
        "\n",
        "- `spaCy` **EntityRuler**: A simpler, more practical method for adding custom, rule-based entities directly into the spaCy pipeline for quick deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbcd04f",
      "metadata": {
        "id": "cdbcd04f"
      },
      "source": [
        "#### We will be covering in this exercise\n",
        "\n",
        "- **CRF Data Preparation**: Preparing training data and IOB format.\n",
        "\n",
        "- **Feature Engineering for CRF**: Defining token features (word shape, context, etc.).\n",
        "\n",
        "- **CRF Model Training (Conceptual)**: Overview of the training and evaluation process.\n",
        "\n",
        "- **Custom NER with spaCy EntityRuler**: Implementing a rule-based pipeline.\n",
        "\n",
        "- **Deployment and Application**: Applying the custom rules to test sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef480bfe",
      "metadata": {
        "id": "ef480bfe"
      },
      "source": [
        "#### What we expect to learn from this exercise\n",
        "\n",
        "- Custom NER is needed when standard spaCy labels (PERSON, ORG) don't cover your domain-specific terms (e.g., PRODUCT, DRUGNAME).\n",
        "\n",
        "- CRF models excel at sequence labelling by modeling transitions between IOB states, relying heavily on hand-crafted features.\n",
        "\n",
        "- Feature Engineering is the most critical step for CRF performance.\n",
        "\n",
        "- spaCy's EntityRuler is the quickest way to deploy custom NER rules within an existing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HNO-s2QYXRpk",
      "metadata": {
        "id": "HNO-s2QYXRpk"
      },
      "source": [
        "**Let's get started**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b76085",
      "metadata": {
        "id": "96b76085"
      },
      "source": [
        "#### Setup and Pre-requisites:\n",
        "\n",
        "For the CRF component, you would typically use `pycrfsuite` or `sklearn-crfsuite`. We will also use `spaCy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499d7a39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "499d7a39",
        "outputId": "cc4419b4-7183-4fb5-c2ed-eafff5520a3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pycrfsuite (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pycrfsuite\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (3.6.0)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.11 sklearn-crfsuite-0.5.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Required libraries for CRF (conceptual part)\n",
        "\n",
        "! pip install pycrfsuite\n",
        "! pip install sklearn-crfsuite\n",
        "\n",
        "# Required library for spaCy pipeline\n",
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9761a0a6",
      "metadata": {
        "id": "9761a0a6"
      },
      "source": [
        "**Preparing Training Data (IOB Format)**: The training data for sequence models like CRF must be in a token-label format (or IOB format), usually as a list of lists of (word, label) pairs, where each inner list represents a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nnx9vCG3ZHNv",
      "metadata": {
        "id": "nnx9vCG3ZHNv"
      },
      "source": [
        "We will use a simple dataset to understand these concepts:\n",
        "```\n",
        "1. I bought the Giga Phone for 999 USD today.\n",
        "2. She uses Nova Pad daily.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u8dGChBFZe4F",
      "metadata": {
        "id": "u8dGChBFZe4F"
      },
      "source": [
        "Next, we will need to parse the training data in I-O-B format as explored in the last challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9c318969",
      "metadata": {
        "id": "9c318969"
      },
      "outputs": [],
      "source": [
        "# Sample IOB-formatted training data (simplified for demonstration)\n",
        "# Entities: PRODUCT (e.g., 'Giga_Phone') and PRICE\n",
        "TRAIN_DATA_IOB = [\n",
        "    [(\"I\", \"O\"), (\"bought\", \"O\"), (\"the\", \"O\"), (\"Giga\", \"B-PRODUCT\"), (\"Phone\", \"I-PRODUCT\"), (\"for\", \"O\"), (\"999\", \"B-PRICE\"), (\"USD\", \"I-PRICE\"), (\"today\", \"O\"), (\".\", \"O\")],\n",
        "    [(\"She\", \"O\"), (\"uses\", \"O\"), (\"the\", \"O\"), (\"Nova\", \"B-PRODUCT\"), (\"Pad\", \"I-PRODUCT\"), (\"daily\", \"O\"), (\".\", \"O\")]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab3b6be",
      "metadata": {
        "id": "cab3b6be"
      },
      "source": [
        "#### Conditional Random Fields (CRF) Implementation\n",
        "\n",
        "A **Conditional Random Field (CRF)** is a discriminative probabilistic model used for labeling or parsing sequential data, such as finding named entities in text. Unlike Hidden Markov Models (**HMM**s), **CRF**s model the conditional probability of the label sequence given the entire observation sequence (the words), which helps avoid the \"label bias problem.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6826d38",
      "metadata": {
        "id": "d6826d38"
      },
      "source": [
        "##### Feature Engineering for CRF\n",
        "\n",
        "For CRF to work effectively, we must manually extract features for every token. These features guide the model in deciding whether a token marks the start (B-), continuation (I-), or non-entity (O) of a named entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2aed5cab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aed5cab",
        "outputId": "72d1c1e9-b542-4282-b7b6-b7f30988a0a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Example Feature Extraction for 'Giga' (Index 3) ---\n",
            "word           : Giga\n",
            "is_start       : False\n",
            "is_end         : False\n",
            "is_capitalized : True\n",
            "is_all_caps    : False\n",
            "is_digit       : False\n",
            "word_shape     : Xxxx\n",
            "prefix_3       : Gig\n",
            "suffix_3       : iga\n",
            "prev_word      : the\n",
            "prev_is_all_caps: False\n",
            "next_word      : Phone\n",
            "next_is_capitalized: True\n"
          ]
        }
      ],
      "source": [
        "# Function to extract features for a token at a given index (i) in a sentence (sent)\n",
        "def extract_features(sent, i):\n",
        "    word = sent[i][0]\n",
        "\n",
        "    # 1. Base Features\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_start': i == 0,\n",
        "        'is_end': i == len(sent) - 1,\n",
        "        'is_capitalized': word.istitle(),\n",
        "        'is_all_caps': word.isupper(),\n",
        "        'is_digit': word.isdigit(),\n",
        "        'word_shape': re.sub(r'[A-Z]', 'X', re.sub(r'[a-z]', 'x', re.sub(r'[0-9]', 'd', word))),\n",
        "        'prefix_3': word[:3],\n",
        "        'suffix_3': word[-3:],\n",
        "    }\n",
        "\n",
        "    # 2. Context Features (Word before the target)\n",
        "    if i > 0:\n",
        "        prev_word = sent[i-1][0]\n",
        "        features.update({\n",
        "            'prev_word': prev_word,\n",
        "            'prev_is_all_caps': prev_word.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True # Beginning of Sentence\n",
        "\n",
        "    # 3. Context Features (Word after the target)\n",
        "    if i < len(sent) - 1:\n",
        "        next_word = sent[i+1][0]\n",
        "        features.update({\n",
        "            'next_word': next_word,\n",
        "            'next_is_capitalized': next_word.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True # End of Sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example Feature Extraction (Need to import 're')\n",
        "import re\n",
        "print(\"--- Example Feature Extraction for 'Giga' (Index 3) ---\")\n",
        "# 'Giga' is the 4th word in the first sentence\n",
        "sample_sent = TRAIN_DATA_IOB[0]\n",
        "giga_features = extract_features(sample_sent, 3)\n",
        "for k, v in giga_features.items():\n",
        "    print(f\"{k:15}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0db2a2cf",
      "metadata": {
        "id": "0db2a2cf"
      },
      "source": [
        "#### CRF Training and Evaluation\n",
        "\n",
        "To train the CRF model, you would convert your IOB data into feature vectors and label sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6df5a5bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6df5a5bc",
        "outputId": "2e198cfe-a5d0-4781-d70e-825ff13ebab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence: [('I', 'O'), ('bought', 'O'), ('the', 'O'), ('Giga', 'B-PRODUCT'), ('Phone', 'I-PRODUCT'), ('for', 'O'), ('999', 'B-PRICE'), ('USD', 'I-PRICE'), ('today', 'O'), ('.', 'O')]\n",
            "sentence: [('She', 'O'), ('uses', 'O'), ('the', 'O'), ('Nova', 'B-PRODUCT'), ('Pad', 'I-PRODUCT'), ('daily', 'O'), ('.', 'O')]\n",
            "0.5952380952380952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-PRODUCT       0.00      0.00      0.00         1\n",
            "   I-PRODUCT       0.00      0.00      0.00         1\n",
            "           O       0.71      1.00      0.83         5\n",
            "\n",
            "    accuracy                           0.71         7\n",
            "   macro avg       0.24      0.33      0.28         7\n",
            "weighted avg       0.51      0.71      0.60         7\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- TRAINING CODE ---\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn_crfsuite.metrics import flatten\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def extract_features_for_sentence(sent):\n",
        "    \"\"\"\n",
        "    Takes a sentence (list of (word, label) tuples) and extracts features for every token.\n",
        "    \"\"\"\n",
        "    return [extract_features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "# Example output for the first three tokens of a sentence (conceptual):\n",
        "# [\n",
        "#     {'word': 'I', 'is_start': True, 'prefix_3': 'I', 'BOS': True, ...},\n",
        "#     {'word': 'bought', 'prev_word': 'I', 'prefix_3': 'bou', ...},\n",
        "#     {'word': 'the', 'prev_word': 'bought', 'prefix_3': 'the', ...},\n",
        "#     # ... and so on\n",
        "# ]\n",
        "\n",
        "\n",
        "def get_labels_for_sentence(sent):\n",
        "    \"\"\"\n",
        "    Takes a sentence (list of (word, label) tuples) and extracts the labels (y values).\n",
        "    \"\"\"\n",
        "    print(f\"sentence: {sent}\")\n",
        "    return [label for (word, label) in sent]\n",
        "\n",
        "# Example output for the first sentence:\n",
        "# ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-PRICE', 'I-PRICE', 'O', 'O']\n",
        "\n",
        "\n",
        "# 1. Prepare Data for CRF\n",
        "X = [extract_features_for_sentence(s) for s in TRAIN_DATA_IOB]\n",
        "y = [get_labels_for_sentence(s) for s in TRAIN_DATA_IOB]\n",
        "\n",
        "# 2. Split Data (Need more than 3 sentences for a meaningful split, but this demonstrates the structure)\n",
        "if len(X) > 1:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "else:\n",
        "    # Use all data as train/test if corpus is too small (not recommended for real tasks)\n",
        "    X_train, X_test = X, X\n",
        "    y_train, y_test = y, y\n",
        "    print(\"\\nWarning: Using all data for both training and testing due to small corpus size.\")\n",
        "\n",
        "crf = CRF(c1=0.1, c2=0.1, max_iterations=100)\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# --- EVALUATION ---\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "print(metrics.flat_f1_score(y_test, y_pred, average='weighted'))\n",
        "#print(metrics.flat_classification_report(y_test, y_pred))\n",
        "\n",
        "# Optional: pick labels explicitly (often exclude 'O' for NER reports)\n",
        "# labels = [l for l in crf.classes_ if l != 'O']\n",
        "\n",
        "print(classification_report(\n",
        "    flatten(y_test),\n",
        "    flatten(y_pred),\n",
        "    # labels=labels,             # uncomment if you want a specific label order\n",
        "    zero_division=0              # avoids divide-by-zero warnings for rare labels\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2efb788a",
      "metadata": {
        "id": "2efb788a"
      },
      "source": [
        "##### Explanation:\n",
        "\n",
        "The CRF model learns weights for each feature, including transition weights (e.g., the probability of B-PRODUCT being followed by I-PRODUCT). Evaluation uses metrics like F1-score, Precision, and Recall, aggregated across all entity types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cb7af8",
      "metadata": {
        "id": "10cb7af8"
      },
      "source": [
        "#### Custom NER Deployment with spaCy EntityRuler\n",
        "\n",
        "For practical, rule-based custom NER deployment, spaCy's **EntityRuler** is highly effective. It allows us to add custom patterns that run before or after the main statistical NER model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3839327a",
      "metadata": {
        "id": "3839327a"
      },
      "source": [
        "**Defining Custom Entity Rules**\n",
        "\n",
        "We define dictionary patterns using spaCy's token matching syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "80dfb3b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80dfb3b4",
        "outputId": "a3c271d7-8aaa-4d4f-e94a-1ea8eec51d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Version: 3.8.7\n"
          ]
        }
      ],
      "source": [
        "# Load the base spaCy model\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "print(f\"spaCy Version: {spacy.__version__}\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a list of patterns for our custom entities (PRODUCT and PRICE)\n",
        "patterns = [\n",
        "    # Pattern 1: Product names (literal match)\n",
        "    {\"label\": \"PRODUCT\", \"pattern\": \"Giga Phone\"},\n",
        "    {\"label\": \"PRODUCT\", \"pattern\": [{\"LOWER\": \"nova\"}, {\"LOWER\": \"pad\"}]},\n",
        "\n",
        "    # Pattern 2: Price expressions (using token attributes)\n",
        "    {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"ddd\"}, {\"LOWER\": \"usd\"}]}, # e.g., '999 USD'\n",
        "    {\"label\": \"PRICE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"TEXT\": \"$\"}, {\"OP\": \"?\"}]}, # e.g., '10$'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87d2020",
      "metadata": {
        "id": "c87d2020"
      },
      "source": [
        "**Integrating the EntityRuler into the Pipeline**\n",
        "\n",
        "The EntityRuler is added to the spaCy pipeline and configured to overwrite (or combine with) existing NER entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "db432766",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db432766",
        "outputId": "fee0c4b4-fdf3-46c3-9967-6f745fdb1ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- spaCy Pipeline with Custom EntityRuler ---\n",
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n"
          ]
        }
      ],
      "source": [
        "# 1. Create the EntityRuler\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# The new pipeline now includes the 'entity_ruler' before the default 'ner' component\n",
        "print(\"\\n--- spaCy Pipeline with Custom EntityRuler ---\")\n",
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c8a41ff",
      "metadata": {},
      "source": [
        "\n",
        "Here’s what each spaCy pipeline component does and why the order matters:\n",
        "\n",
        "**tok2vec**\n",
        "\n",
        "- Learns contextual token vectors (embeddings) from raw text.\n",
        "- Provides shared features used by downstream components like tagger, parser, and NER.\n",
        "- Runs first so everything else can use these representations.\n",
        "\n",
        "**tagger**\n",
        "\n",
        "- Predicts coarse/fine-grained part‑of‑speech tags (e.g., NOUN, VERB, PROPN) and often morphological features.\n",
        "- Its output helps the parser and lemmatizer.\n",
        "\n",
        "**parser**\n",
        "\n",
        "- Predicts dependency parse (who modifies whom) and sentence boundaries.\n",
        "- Useful for relation extraction and many downstream tasks.\n",
        "\n",
        "**attribute_ruler**\n",
        "\n",
        "- Rule-based assignment of token attributes such as LEMMA, NORM, or TAG based on patterns.\n",
        "- Lets you normalize or fix attributes before statistical lemmatization runs.\n",
        "- Typically comes before the lemmatizer to provide better hints.\n",
        "\n",
        "**lemmatizer**\n",
        "\n",
        "- Reduces words to their base form (e.g., “running” → “run”), using POS/morph info plus rules/lookups.\n",
        "- Depends on the tagger and attribute_ruler for good accuracy.\n",
        "\n",
        "**entity_ruler**\n",
        "\n",
        "- Rule-based NER using patterns (string or token-based) that can create or overwrite entities.\n",
        "- You placed it before 'ner', so its matches are set first; the statistical NER can then keep or adjust them depending on overwrite/ent_id settings.\n",
        "\n",
        "**ner**\n",
        "\n",
        "- Statistical Named Entity Recognizer that predicts entity spans and labels (e.g., PERSON, ORG, DATE).\n",
        "- Benefits from tok2vec features and sometimes parser/tagger signals.\n",
        "Runs after entity_ruler in your setup, so your custom rules prime or override the model’s decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5f63f5",
      "metadata": {
        "id": "9d5f63f5"
      },
      "source": [
        "#### Deployment and Application\n",
        "\n",
        "We apply the new spaCy pipeline, which combines the base NER model with our custom rules, on test sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d7360c58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "d7360c58",
        "outputId": "0ea21a9b-792c-47f4-c530-eff2f7ef1fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Custom NER Application (EntityRuler) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I prefer the Nova Pad over the old \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Giga Phone\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", which costs $\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    100\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " less.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity               | Label     \n",
            "-----------------------------------\n",
            "Giga Phone           | ORG       \n",
            "100                  | MONEY     \n",
            "\n",
            "--- IOB Tags with Custom Entities ---\n",
            "Token      | Full IOB Tag\n",
            "-------------------------\n",
            "I          | O\n",
            "prefer     | O\n",
            "the        | O\n",
            "Nova       | O\n",
            "Pad        | O\n",
            "over       | O\n",
            "the        | O\n",
            "old        | O\n",
            "Giga       | B-ORG\n",
            "Phone      | I-ORG\n",
            ",          | O\n",
            "which      | O\n",
            "costs      | O\n",
            "$          | O\n",
            "100        | B-MONEY\n",
            "less       | O\n",
            ".          | O\n"
          ]
        }
      ],
      "source": [
        "# Test sentences to evaluate custom NER\n",
        "#--- spaCy Pipeline with Custom EntityRuler ---\n",
        "# ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "TEST_SENTENCE_1 = \"I prefer the Nova Pad over the old Giga Phone, which costs $100 less.\"\n",
        "TEST_SENTENCE_2 = \"The CEO of Google announced the product on Tuesday.\" # Test overlap with base NER\n",
        "\n",
        "doc_test = nlp(TEST_SENTENCE_1)\n",
        "\n",
        "print(\"\\n--- Custom NER Application (EntityRuler) ---\")\n",
        "#displacy.render(doc_test, style=\"ent\", jupyter=True)\n",
        "\n",
        "html = displacy.render(doc_test, style=\"ent\", jupyter=False)\n",
        "display(HTML(html))\n",
        "\n",
        "# Programmatic extraction to verify custom labels\n",
        "print(f\"{'Entity':20} | {'Label':10}\")\n",
        "print(\"-\" * 35)\n",
        "for ent in doc_test.ents:\n",
        "    print(f\"{ent.text:20} | {ent.label_:10}\")\n",
        "\n",
        "# Example of IOB tags with custom entities:\n",
        "print(\"\\n--- IOB Tags with Custom Entities ---\")\n",
        "print(f\"{'Token':10} | {'Full IOB Tag'}\")\n",
        "print(\"-\" * 25)\n",
        "for token in doc_test:\n",
        "    iob_prefix = token.ent_iob_\n",
        "    entity_type = token.ent_type_\n",
        "\n",
        "    if iob_prefix == 'O':\n",
        "        full_iob_tag = 'O'\n",
        "    else:\n",
        "        full_iob_tag = f\"{iob_prefix}-{entity_type}\"\n",
        "\n",
        "    print(f\"{token.text:10} | {full_iob_tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4232473a",
      "metadata": {
        "id": "4232473a"
      },
      "source": [
        "#### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c585fb6e",
      "metadata": {
        "id": "c585fb6e"
      },
      "source": [
        "This exercise, we explored the core concepts required for building a Custom NER system.\n",
        "\n",
        "We first detailed the intensive data preparation, feature engineering, and training process needed for the traditional Conditional Random Fields (CRF) model. While CRFs provide highly accurate sequence modeling, they require significant manual effort in feature design.\n",
        "\n",
        "We then showed the practical way to deploy custom rules within a modern framework by using spaCy's EntityRuler. This method is fast, easy to maintain, and seamlessly integrates with the existing statistical NER model, making it the preferred approach for quickly injecting domain-specific knowledge into an NLP pipeline. Custom NER is essential for extracting targeted, domain-specific information from text.        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XKBgGzT9YkS9",
      "metadata": {
        "id": "XKBgGzT9YkS9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai-lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
