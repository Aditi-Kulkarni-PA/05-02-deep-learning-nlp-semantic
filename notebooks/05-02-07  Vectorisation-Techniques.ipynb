{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0d4f45",
   "metadata": {
    "id": "6f0d4f45"
   },
   "source": [
    "### Vectorisation Techniques: CountVectorizer, BoW, and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e415915",
   "metadata": {
    "id": "7e415915"
   },
   "source": [
    "In this exercise we will explore the fundamental techniques used to convert raw, unstructured text into a numerical format that machine learning models can process. This process is known as vectorisation or feature extraction. We will implement the Bag of Words (BoW) model using CountVectorizer and extend this concept to Term Frequency-Inverse Document Frequency (TF-IDF) using TfidfVectorizer. We will compare these two methods to illustrate how TF-IDF weighting prioritizes important, domain-specific words over common terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f851415",
   "metadata": {
    "id": "2f851415"
   },
   "source": [
    "#### What we will discuss in this notebook\n",
    "\n",
    "- **The Bag of Words (BoW) Concept**: Manual illustration of word counting.\n",
    "\n",
    "- **CountVectorizer Implementation (BoW)**: Creating a document-term matrix.\n",
    "\n",
    "- **TfidfVectorizer Implementation (TF-IDF)**: Assigning importance weights.\n",
    "\n",
    "- **Comparative Analysis**: Contrasting BoW counts with TF-IDF scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f778d",
   "metadata": {
    "id": "189f778d"
   },
   "source": [
    "#### What we will learn from this exercise:\n",
    "\n",
    "- **Vectorization** is essential for all text-based machine learning.\n",
    "\n",
    "- The **Bag of Words (BoW)** model represents text as the frequency of words, ignoring grammar and word order.\n",
    "\n",
    "- **CountVectorizer** efficiently implements the BoW model, creating a Document-Term Matrix.\n",
    "\n",
    "- **TF-IDF** is a weighting scheme that penalizes common words (like \"the\") and rewards rare, distinctive words, reflecting their importance in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ff6a4",
   "metadata": {
    "id": "998ff6a4"
   },
   "source": [
    "#### Now, let us get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867290a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "867290a8",
    "outputId": "377e6a73-4cb8-476a-b74b-2cdbdb6bf10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn Version: 1.7.2\n",
      "Pandas Version: 2.3.3\n",
      "\n",
      "--- Sample Corpus ---\n",
      "Document 1: The sun is shining brightly today.\n",
      "Document 2: The car is a fast car and the sun is hot.\n",
      "Document 3: A sunny day and a hot sun make me happy.\n",
      "Document 4: I drive a fast car on a sunny road.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "print(f\"scikit-learn Version: {sklearn.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "\n",
    "# Sample Corpus (Document collection)\n",
    "CORPUS = [\n",
    "    \"The sun is shining brightly today.\",\n",
    "    \"The car is a fast car and the sun is hot.\",\n",
    "    \"A sunny day and a hot sun make me happy.\",\n",
    "    \"I drive a fast car on a sunny road.\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sample Corpus ---\")\n",
    "for i, doc in enumerate(CORPUS):\n",
    "    print(f\"Document {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70dab0",
   "metadata": {
    "id": "9d70dab0"
   },
   "source": [
    "#### The Bag of Words (BoW) Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s17a3P6frW5h",
   "metadata": {
    "id": "s17a3P6frW5h"
   },
   "source": [
    "The Bag of Words (BoW) model is a simple representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "1. A vocabulary of all known words.\n",
    "\n",
    "2. The frequency (count) of each word in the document.\n",
    "\n",
    "The BoW model is called a \"bag\" because it completely **disregards word order** and grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BOY3gd8erljO",
   "metadata": {
    "id": "BOY3gd8erljO"
   },
   "source": [
    "We demonstrate the BoW concept by calculating word counts for a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3vXo3tV_q5w2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vXo3tV_q5w2",
    "outputId": "5f78c081-6a07-402f-eb1e-ba2b23e07d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Manual BoW for Document 2 ---\n",
      "Document: 'The car is a fast car and the sun is hot.'\n",
      "Word Counts: {'the': 2, 'car': 2, 'is': 2, 'a': 1, 'fast': 1, 'and': 1, 'sun': 1}\n"
     ]
    }
   ],
   "source": [
    "sample_doc = CORPUS[1] # \"The car is a fast car and the sun is hot.\"\n",
    "\n",
    "# Simple manual tokenization and counting (ignoring punctuation)\n",
    "word_count_dict = {}\n",
    "for word in sample_doc.lower().split():\n",
    "    # Only count alphanumeric words for simplicity\n",
    "    if word.isalnum():\n",
    "        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n",
    "\n",
    "print(f\"\\n--- Manual BoW for Document 2 ---\")\n",
    "print(f\"Document: '{sample_doc}'\")\n",
    "print(f\"Word Counts: {word_count_dict}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wYqYSjw6rp4e",
   "metadata": {
    "id": "wYqYSjw6rp4e"
   },
   "source": [
    "The words 'the', 'is', and 'car' appear multiple times. The BoW representation for this document is simply the list of (word, count) pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3P3yvKA-r1vS",
   "metadata": {
    "id": "3P3yvKA-r1vS"
   },
   "source": [
    "### CountVectorizer Implementation\n",
    "\n",
    "CountVectorizer from scikit-learn automates the BoW process by tokenizing the text and building the vocabulary. The output is a sparse Document-Term Matrix where rows are documents and columns are unique words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pyt968xdsANg",
   "metadata": {
    "id": "pyt968xdsANg"
   },
   "source": [
    "#### Creating the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8VA5-N7isDib",
   "metadata": {
    "id": "8VA5-N7isDib"
   },
   "source": [
    "We apply the vectorizer to our corpus and transform the text into numerical counts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UYnTqygarpEM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYnTqygarpEM",
    "outputId": "a73c79ba-06fe-4b15-b16b-d86c34897849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document-Term Matrix (BoW Counts) ---\n",
      "       brightly  car  day  drive  fast  happy  hot  make  road  shining  sun  \\\n",
      "Doc 1         1    0    0      0     0      0    0     0     0        1    1   \n",
      "Doc 2         0    2    0      0     1      0    1     0     0        0    1   \n",
      "Doc 3         0    0    1      0     0      1    1     1     0        0    1   \n",
      "Doc 4         0    1    0      1     1      0    0     0     1        0    0   \n",
      "\n",
      "       sunny  today  \n",
      "Doc 1      0      1  \n",
      "Doc 2      0      0  \n",
      "Doc 3      1      0  \n",
      "Doc 4      1      0  \n",
      "\n",
      "Vocabulary Size (after stop words removal): 13\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer. We include stop_words removal and lowercase conversion.\n",
    "count_vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "# Fit the vectorizer to the corpus (learn the vocabulary)\n",
    "count_matrix_sparse = count_vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense DataFrame for inspection\n",
    "bow_df = pd.DataFrame(count_matrix_sparse.toarray(), columns=feature_names, index=[f'Doc {i+1}' for i in range(len(CORPUS))])\n",
    "\n",
    "print(\"\\n--- Document-Term Matrix (BoW Counts) ---\")\n",
    "print(bow_df)\n",
    "\n",
    "# Inspection: Vocabulary size\n",
    "print(f\"\\nVocabulary Size (after stop words removal): {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LwKsZ7JssJVU",
   "metadata": {
    "id": "LwKsZ7JssJVU"
   },
   "source": [
    "The matrix shows the raw frequency of each word (column) within each document (row). For example, the word 'sun' appears 1 times in 'Doc 3'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2D4n3dYysdNT",
   "metadata": {
    "id": "2D4n3dYysdNT"
   },
   "source": [
    "### TfidfVectorizer Implementation (TF-IDF)\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a weighting scheme that evaluates how important a word is to a document in a corpus.\n",
    "\n",
    "- Term Frequency (TF): The frequency of a word in the current document (similar to BoW).\n",
    "\n",
    "- Inverse Document Frequency (IDF): Measures how rare the word is across all documents. Words that appear in many documents (like 'sun' in this corpus) receive a lower IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iE6pFZz9slqf",
   "metadata": {
    "id": "iE6pFZz9slqf"
   },
   "source": [
    "$$TF-IDF(t,d,D)=TF(t,d)Ã—IDF(t,D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSgw7-yWs20q",
   "metadata": {
    "id": "jSgw7-yWs20q"
   },
   "source": [
    "#### Creating the TF-IDF Matrix\n",
    "\n",
    "We use TfidfVectorizer, which calculates both the TF and IDF components and outputs the final weighted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S2QY2ha_sObi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T08:00:13.965141Z",
     "start_time": "2025-10-16T08:00:13.756600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2QY2ha_sObi",
    "outputId": "c544ca25-df09-41bc-d154-5641c5cee9f3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize TfidfVectorizer with the same preprocessing steps\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fit and transform the corpus\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tfidf_matrix_sparse \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(CORPUS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer with the same preprocessing steps\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix_sparse = tfidf_vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Convert the sparse matrix to a dense DataFrame for inspection\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_sparse.toarray(), columns=feature_names, index=[f'Doc {i+1}' for i in range(len(CORPUS))])\n",
    "\n",
    "print(\"\\n--- Document-Term Matrix (TF-IDF Weights) ---\")\n",
    "# Rounding for better readability\n",
    "print(tfidf_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define CORPUS inline (safe even after kernel restart)\n",
    "CORPUS = [\n",
    "    \"The sun is shining brightly today.\",\n",
    "    \"The car is a fast car and the sun is hot.\",\n",
    "    \"A sunny day and a hot sun make me happy.\",\n",
    "    \"I drive a fast car on a sunny road.\",\n",
    "]\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix_sparse = tfidf_vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "# Get feature names from this vectorizer\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix_sparse.toarray(),\n",
    "    columns=tfidf_feature_names,\n",
    "    index=[f'Doc {i+1}' for i in range(len(CORPUS))]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Document-Term Matrix (TF-IDF Weights) ---\")\n",
    "print(tfidf_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64JLSF8XtFeb",
   "metadata": {
    "id": "64JLSF8XtFeb"
   },
   "source": [
    "#### Why TF-IDF is more useful than BoW? (Comparative Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MC9dpl5vtO5e",
   "metadata": {
    "id": "MC9dpl5vtO5e"
   },
   "source": [
    "The power of TF-IDF lies in how it suppresses frequent, uninformative words and boosts unique, distinguishing words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kc__TzfOtSNt",
   "metadata": {
    "id": "kc__TzfOtSNt"
   },
   "source": [
    "Let's compare the raw count and the TF-IDF score for a common word ('sun') and a rare word ('drive') in the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YtKEuo3ts9Ru",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtKEuo3ts9Ru",
    "outputId": "0a8a379a-7e2c-4a50-bacc-7bda8189146e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison in Doc 4 ---\n",
      "Word       | BoW Count  | TF-IDF Score   \n",
      "--------------------------------------\n",
      "sun        | 0          | 0.0000         \n",
      "drive      | 1          | 0.5087         \n"
     ]
    }
   ],
   "source": [
    "target_doc = 'Doc 4'\n",
    "common_word = 'sun'\n",
    "rare_word = 'drive'\n",
    "\n",
    "# Get the index of the document\n",
    "doc_index = 3 # Doc 4 is at index 3\n",
    "\n",
    "print(f\"\\n--- Comparison in {target_doc} ---\")\n",
    "\n",
    "# Data for Common Word ('sun')\n",
    "bow_sun = bow_df.loc[target_doc, common_word] if common_word in bow_df.columns else 0\n",
    "tfidf_sun = tfidf_df.loc[target_doc, common_word] if common_word in tfidf_df.columns else 0\n",
    "# TF-IDF penalizes 'sun' because it appears in many documents (low IDF).\n",
    "\n",
    "# Data for Rare Word ('drive')\n",
    "bow_drive = bow_df.loc[target_doc, rare_word] if rare_word in bow_df.columns else 0\n",
    "tfidf_drive = tfidf_df.loc[target_doc, rare_word] if rare_word in tfidf_df.columns else 0\n",
    "# TF-IDF rewards 'drive' because it is unique to Document 4 (high IDF).\n",
    "\n",
    "print(f\"{'Word':10} | {'BoW Count':10} | {'TF-IDF Score':15}\")\n",
    "print(\"-\" * 38)\n",
    "print(f\"{common_word:10} | {bow_sun:<10} | {tfidf_sun:<15.4f}\")\n",
    "print(f\"{rare_word:10} | {bow_drive:<10} | {tfidf_drive:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxXLIHd-tWv5",
   "metadata": {
    "id": "rxXLIHd-tWv5"
   },
   "source": [
    "The raw count for 'sun' might be 1 (or 0 in Doc 4, but let's assume it was 1), but its TF-IDF score is significantly reduced due to its high corpus frequency (low IDF). The word 'drive' has a count of 1, but its TF-IDF score is high because it is rare and thus highly distinctive to Doc 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kq1fvJpotyqp",
   "metadata": {
    "id": "Kq1fvJpotyqp"
   },
   "source": [
    "#### Matrix Sparsity\n",
    "\n",
    "Both BoW and TF-IDF matrices are typically sparse, meaning most entries are zero. This is efficient for storage but requires specialised data structures (like those used in the sparse matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dL9kDR9htWHh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL9kDR9htWHh",
    "outputId": "4ab0b3b0-dd48-4ef2-cc26-5e7cbe43a8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparsity of Count Matrix: 0.00000\n",
      "Sparsity of TF-IDF Matrix: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity (proportion of non-zero elements)\n",
    "sparsity_count = 1.0 - (count_matrix_sparse.count_nonzero() / count_matrix_sparse.size)\n",
    "sparsity_tfidf = 1.0 - (tfidf_matrix_sparse.count_nonzero() / tfidf_matrix_sparse.size)\n",
    "\n",
    "print(f\"\\nSparsity of Count Matrix: {sparsity_count:.5f}\")\n",
    "print(f\"Sparsity of TF-IDF Matrix: {sparsity_tfidf:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1TsJbr_at5yj",
   "metadata": {
    "id": "1TsJbr_at5yj"
   },
   "source": [
    "The matrices are sparse because most words do not appear in every document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usEuk5xEuWj8",
   "metadata": {
    "id": "usEuk5xEuWj8"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ko6uFqmHuRpn",
   "metadata": {
    "id": "ko6uFqmHuRpn"
   },
   "source": [
    "| Feature     | BoW / CountVectorizer                          | TF-IDF / TfidfVectorizer                          |\n",
    "|-------------|------------------------------------------------|--------------------------------------------------|\n",
    "| Value       | Raw count of word occurrences.                 | Weighted score reflecting importance.            |\n",
    "| Strength    | Simplicity, ease of implementation.            | Captures word importance, better for classification. |\n",
    "| Weakness    | Frequent words (noise) dominate scores.        | Computationally more complex.                    |\n",
    "| Application | Simple document clustering, baseline models.   | Text classification, information retrieval ranking. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zJSWVbKCuais",
   "metadata": {
    "id": "zJSWVbKCuais"
   },
   "source": [
    "**Final Note**: Vectorisation successfully converts text into numerical vectors, with each word acting as a feature. This numerical representation is the required input for all subsequent unsupervised and supervised machine learning tasks in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5EAKws0rt4sp",
   "metadata": {
    "id": "5EAKws0rt4sp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
